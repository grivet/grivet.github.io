<!doctype html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" href="data:,"/>
    <title>U256 | A fast lock-less MPSC queue</title>
    <link rel="stylesheet" href="../style.css"/>

    
</head>
<body>
    <header>
        <div>
            <a href="../">Home</a>
        </div>
<div id='navbar'>
<span><a href="../notes">notes</a></span>
<span><a href="./">posts</a></span>
</div>
    </header>

<main>
    <div class="title">
        <h1>A fast lock-less MPSC queue</h1>
        <time datetime="2021-02-21">2021-02-21</time>
    </div>
<blockquote>
<p>Code available on <a href="https://github.com/grivet/mpsc-queue">github</a>.</p>
</blockquote>
<p>Sharing mutable data kills concurrency.</p>
<p>Expanding on this basic idea, rules can be derived to build a concurrent
system, hopefully keeping things simple:</p>
<ul>
<li>Mutable data that is not shared is <em>isolated</em>.</li>
<li>A simple way to isolate data is to only allow a single reference to it.</li>
<li>If mutable data needs to be used by more than one actors, data ownership changes
must be safely signaled.</li>
<li>Holding the single reference to the data is a good token for ownership,
and it can be passed among actors as a message.</li>
</ul>
<p>In such a concurrent system, the message queue would be a critical primitive.</p>
<p>I found the <a href="http://www.1024cores.net/home/lock-free-algorithms/queues/intrusive-mpsc-node-based-queue">queue described by Dmitri Vyukov</a> simple and practical if cooperative
threads are used. It is an MPSC queue, which limits the constraints and the need
for synchronization. This in turn allows keeping it light and performant.</p>
<p>It offers interesting properties but has important caveats that should be highlighted.</p>
<h2>Taxonomy</h2>
<p>This message queue is an intrusive linked-list, meaning that a queue node is part of the element
being inserted in the list. As a result the element itself is modified by queue operations.
This is a good fit for low latency systems, as it avoids one additional dereference and reduces
heap fragmentation. While generally linked-lists are terrible for cache locality, they are sometimes
unavoidable. In that case, an intrusive list is better.</p>
<p>Queue ordering is per-producer FIFO, i.e. it keeps the order of insertions from each producer point of view.
However, between producers no ordering is guaranteed, unless additional synchronization is used.</p>
<p>Insertion is done in a finite and constant number of steps by any number of producers.
State synchronization is done at a single point using an atomic exchange. This is lighter than
compare-exchange based queues as there is no loop to resolve concurrent writes.</p>
<p>Reading the queue state and removing elements is limited to a single consumer at a time.
Peeking at the tail of the queue is done in a finite and constant number of steps, like insertion.
Removal however requires producers to complete their writes (more on why below), making the consumer
dependent on other threads forward-progress.</p>
<p>As insertion and removal are both <strong>exchange</strong> based, instead of <strong>compare-exchange</strong> based, they are
immune from the <em>ABA</em> problem. There is no need to prevent it, further simplifying operations.</p>
<p>Insertion and peeking are finite and constant, but removal, while resolved in constant time as well (either
for success or failure) is dependent on other threads progressing. This makes the queue obstruction-free
only, the weakest forward-progress guarantee.</p>
<p>Such structure relies on cooperative peers to avoid livelock. This might not be an issue for some
concurrent systems, but it is important to keep in mind.</p>
<p>This queue can be useful for low-latency systems, such as <em>thread-per-core</em> applications.
It could also be used to implement the <a href="https://en.wikipedia.org/wiki/Actor_model">Actor model</a> for example.</p>
<h2>Operations</h2>
<h3>Insertion</h3>
<p>The queue is implemented using a singly-linked list. Insertion is done at the
back of the queue. First the current end is swapped with the new node atomically.
The previous end is then updated to point to the new node. To follow Vyukov's
nomenclature, the end-node of the chain is the head. A producer will
only manipulate the head.</p>
<pre><code>
Before: the queue is in a consistent state and its end currently points to a node 'C'.
Thread 1 is going to insert a node 'D'.

      ,-----.   ,-----.   ,-----.                   ,-----.
- - -&gt;|  A  +--&gt;|  B  +--&gt;|  C  +--&gt; //             |  D  +--&gt; //
      `-----'   `-----'   `-----'                   `1----'
                             ^
  head +---------------------'


Insertion part A: the head pointer is swapped by thread 1, pointing to 'D'.

      ,-----.   ,-----.   ,-----.       ,-----.
- - -&gt;|  A  +--&gt;|  B  +--&gt;|  C  +--&gt; // |  D  +--&gt; //
      `-----'   `-----'   `-----'       `1----'
                                           ^
  head +-----------------------------------'


Insertion part B: thread 1 links 'C' to 'D'.

      ,-----.   ,-----.   ,-----.   ,-----.
- - -&gt;|  A  +--&gt;|  B  +--&gt;|  C  +--&gt;|  D  +--&gt; //
      `-----'   `-----'   `1----'   `-----'
                                       ^
  head +-------------------------------'

</code></pre>
<p>The head swap is atomic, however the link from the previous head to the new
one is done in a separate operation. Any number of threads can execute the first
part of insertion concurrently. The head can be moved around several
times in any execution order, and it will remain correct in the sense that it
will still point to the last enqueued element.</p>
<p>Closing the chain however is done in a second, separate operation.
Until the chain is closed, it remains in an inconsistent state.
In that state, the previous seen head (C in this example) still points to <code>NULL</code>,
but the next published head is D.</p>
<pre><code>
Multiple threads inserting concurrently:

      ,-----.   ,-----.   ,-----.       ,-----.   ,-----.       ,-----.   ,-----.
- - -&gt;|  A  +--&gt;|  B  +--&gt;|  C  +--&gt; // |  D  +--&gt;|  E  +--&gt; // |  F  +--&gt;|  G  +--&gt; //
      `-----'   `-----'   `-----'       `1----'   `2----'       `3----'   `4----'
                                                                             ^
  head +---------------------------------------------------------------------'


The consumer can only read until 'C', waiting on thread 1 to finish the insertion.
Threads 2 and 4 have finished inserting. Threads 1 and 3 are between part A and B.

</code></pre>
<p>Considering a series of insertions, the queue state will remain consistent
and their order is compatible with their precedence, thus this operation
is serializable. However, because an insertion consists in two separate memory
transactions, it is not linearizable.</p>
<h3>Removal</h3>
<p>The consumer must deal with the queue inconsistency. It will manipulate
the tail of the queue and move it along the latest consumed elements.</p>
<pre><code>
Consumer reads from the tail:

        ,------.   ,-----.   ,-----.   ,-----.
        | stub +--&gt;|  A  +--&gt;|  B  +--&gt;|  C  +--&gt; //
        `------'   `-----'   `-----'   `-----'
           ^                              ^
  tail +---'                              |
  head +----------------------------------'

The tail first points to stub. As it is a known sentinel,
it is skipped and 'A' is returned, then 'B', then 'C'.

</code></pre>
<p>When an end of the chain of elements is found (the next pointer is <code>NULL</code>),
the tail is compared with the head.</p>
<p>If they point to different addresses, then the queue is in an inconsistent
state: the tail cannot move forward as the next is <code>NULL</code>, but the head is not
the last element in the chain: this can only happen if the chain is broken.</p>
<pre><code>
Reading in inconsistent state:

        ,------.   ,-----.   ,-----.   ,-----.       ,-----.
        | stub +--&gt;|  A  +--&gt;|  B  +--&gt;|  C  +--&gt; // |  D  +--&gt; //
        `------'   `-----'   `-----'   `-----'       `-----'
                                          ^             ^
  tail +----------------------------------'             |
  head +------------------------------------------------'

As the tail cannot go past 'C', if the head and tail points to different nodes,
the queue is known to be inconsistent.

</code></pre>
<p>In this case, the consumer must wait for the producer to finish writing the
next pointer of its current tail.</p>
<p>Removal is in most cases (when there are elements in the queue)
accomplished without using atomics, until the last element of the queue.
There, the head is atomically loaded. If the queue is in a consistent state,
the head is moved back to the queue stub by inserting the latter in the queue:
ending the queue is the same as an insertion, which is one atomic exchange.</p>
<pre><code>
Emptying the queue in a consistent state:

The consumer read 'D' already and tries to get the next node.

        ,------.   ,-----.   ,-----.   ,-----.   ,-----.
        | stub +--&gt;|  A  +--&gt;|  B  +--&gt;|  C  +--&gt;|  D  +--&gt; //
        `------'   `-----'   `-----'   `-----'   `-----'
                                                    ^^
  tail +--------------------------------------------'|
  head +---------------------------------------------'

As head and tail are equal, insert the queue stub:

        ,------.       ,-----.   ,-----.   ,-----.   ,-----.   ,------.
        | stub +--&gt; // |  A  +--&gt;|  B  +--&gt;|  C  +--&gt;|  D  +--&gt;| stub +--&gt; //
        `------'       `-----'   `-----'   `-----'   `-----'   `------'
                                                        ^         ^
  tail +------------------------------------------------'         |
  head +----------------------------------------------------------'

        ,------.
        | stub +--&gt; //
        `------'
           |
  tail +---'
  head +---'

Next 'read', skip the stub and return NULL: the queue is empty.

</code></pre>
<h3>Safety</h3>
<p>Because the queue can be in transient inconsistent states, the reader needs
to peek before removing an element. Finding a consistent state is dependent on
producers completing their writes.</p>
<p>If a producer was suspended after exchanging the head, but before linking to the new one,
the consumer is blocked from reading until the producer resumes. If the producer is cancelled,
then the consumer will never be able to access the remaining entries.</p>
<p>If the producers are very rarely or never suspended, then inconsistent states should
be short and rare. Producers should never insert while being cancellable.</p>
<h2>Usage</h2>
<p>The queue is lockless. To benefit from this property, ideally neither producers nor
consumer should have to take a lock to use it. Unfortunately, this means that there is
no proper way to signal when new elements are available in the queue: the consumer
thread is forced to poll it for new items.</p>
<p>Such polling can be implemented using <strong>exponential backoff</strong> if no other work is expected.
It should reduce useless activity of the consumer thread, at the price of some latency
for the first new element if activity picks up.</p>
<p>This is the reason I named the removal function <code>mpsc_queue_poll</code>. It does not guarantee
to return an element, but will signal when it does.</p>
<p>Also, to properly implement exponential backoff, the consumer thread must differentiate between
seeing the queue empty or in an inconsistent state. Instead of a binary <code>NULL</code>/<code>addr</code> returned for
an item, I defined three polling results:</p>
<ul>
<li><code>MPSC_QUEUE_EMPTY</code>, which means that the queue has no elements and backoff could start.</li>
<li><code>MPSC_QUEUE_ITEM</code>, in which case the given pointer is set to the removed element.</li>
<li><code>MPSC_QUEUE_RETRY</code>, signaling that a producer has not yet finished writing and polling should
be attempted again soon. No need to start backoff.</li>
</ul>
<h2>Performance</h2>
<p>The repository linked with this post contains a small benchmark comparing
Vyukov queue to other MPSC queues. This post being too long already, I will
leave actual benchmark for another time, but it is clear that the queue simplicity
makes it interesting, especially as the number of core grows.</p>
</main>

</body>
</html>
